{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3SFEnTvs/qvgBXiGgJMU1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_z8XI6yHbdv"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/sjiang83/fomc-ood-stress-test.git\n",
        "#%cd fomc-ood-stress-test\n",
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Notebook 1: Data Collection & Cleaning (2024-2025 FOMC Statements)\n",
        "Purpose: Load, parse, and segment FOMC statements for model-ready processing\n",
        "Author: Shanhuizi (Mia) Jiang\n",
        "GitHub: https://github.com/sjiang83/fomc-ood-stress-test\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================\n",
        "# SETUP: Import Dependencies\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# Optional: For future parsing improvements\n",
        "# from bs4 import BeautifulSoup  # If scraping HTML versions\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Notebook 1: Data Collection & Cleaning\")\n",
        "print(\"Target: 2024-2025 FOMC Statements (OOD Data)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "# Project paths\n",
        "PROJECT_ROOT = Path(\"..\").resolve()  # Assumes notebook is in notebooks/\n",
        "RAW_DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "PROCESSED_DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "\n",
        "# Ensure processed directory exists\n",
        "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Raw data directory: {RAW_DATA_DIR}\")\n",
        "print(f\"üìÅ Processed data directory: {PROCESSED_DATA_DIR}\")\n",
        "print()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: Load Raw FOMC Statements\n",
        "# ============================================================\n",
        "\n",
        "def load_fomc_statements(data_dir):\n",
        "    \"\"\"\n",
        "    Load all FOMC statement text files from raw data directory.\n",
        "\n",
        "    Args:\n",
        "        data_dir (Path): Directory containing raw FOMC .txt files\n",
        "\n",
        "    Returns:\n",
        "        dict: {filename: text_content}\n",
        "    \"\"\"\n",
        "    statements = {}\n",
        "    txt_files = sorted(data_dir.glob(\"fomc_*.txt\"))\n",
        "\n",
        "    if not txt_files:\n",
        "        print(\"‚ö†Ô∏è  No FOMC statement files found!\")\n",
        "        print(f\"   Expected pattern: fomc_YYYY_MM_DD_statement.txt in {data_dir}\")\n",
        "        return statements\n",
        "\n",
        "    print(f\"Found {len(txt_files)} FOMC statement files:\")\n",
        "\n",
        "    for txt_file in txt_files:\n",
        "        try:\n",
        "            with open(txt_file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "            statements[txt_file.name] = content\n",
        "\n",
        "            # Extract date from filename for display\n",
        "            date_match = re.search(r'(\\d{4})_(\\d{2})_(\\d{2})', txt_file.name)\n",
        "            if date_match:\n",
        "                year, month, day = date_match.groups()\n",
        "                print(f\"  ‚úì {year}-{month}-{day}: {len(content)} characters\")\n",
        "            else:\n",
        "                print(f\"  ‚úì {txt_file.name}: {len(content)} characters\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚úó Failed to load {txt_file.name}: {e}\")\n",
        "\n",
        "    return statements\n",
        "\n",
        "# Load statements\n",
        "raw_statements = load_fomc_statements(RAW_DATA_DIR)\n",
        "print(f\"\\n‚úì Loaded {len(raw_statements)} statements successfully\")\n",
        "print()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: Basic Text Cleaning\n",
        "# ============================================================\n",
        "\n",
        "def clean_fomc_text(text):\n",
        "    \"\"\"\n",
        "    Basic cleaning for FOMC statement text.\n",
        "\n",
        "    Operations:\n",
        "    - Remove extra whitespace\n",
        "    - Normalize line breaks\n",
        "    - Remove page numbers/headers (if present)\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw FOMC statement text\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    # Remove common headers/footers patterns\n",
        "    text = re.sub(r'For (immediate )?release.*?\\n', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Multiple newlines -> double newline\n",
        "    text = re.sub(r' +', ' ', text)  # Multiple spaces -> single space\n",
        "\n",
        "    # Remove leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 2: Cleaning Text\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cleaned_statements = {}\n",
        "for filename, content in raw_statements.items():\n",
        "    cleaned = clean_fomc_text(content)\n",
        "    cleaned_statements[filename] = cleaned\n",
        "\n",
        "    # Show cleaning stats\n",
        "    original_lines = len(content.split('\\n'))\n",
        "    cleaned_lines = len(cleaned.split('\\n'))\n",
        "    print(f\"{filename}: {original_lines} ‚Üí {cleaned_lines} lines\")\n",
        "\n",
        "print(f\"\\n‚úì Cleaned {len(cleaned_statements)} statements\")\n",
        "print()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3: Sentence Segmentation\n",
        "# ============================================================\n",
        "\n",
        "def segment_into_sentences(text):\n",
        "    \"\"\"\n",
        "    Segment FOMC statement into sentences for model inference.\n",
        "\n",
        "    Uses simple rule-based approach:\n",
        "    - Split on periods followed by space and capital letter\n",
        "    - Handle common abbreviations (e.g., \"U.S.\", \"etc.\")\n",
        "\n",
        "    Args:\n",
        "        text (str): Cleaned FOMC statement text\n",
        "\n",
        "    Returns:\n",
        "        list: List of sentence strings\n",
        "    \"\"\"\n",
        "    # Protect common abbreviations\n",
        "    text = text.replace('U.S.', 'U_S_')\n",
        "    text = text.replace('e.g.', 'e_g_')\n",
        "    text = text.replace('i.e.', 'i_e_')\n",
        "\n",
        "    # Simple sentence split: period + space + capital letter\n",
        "    sentences = re.split(r'\\.\\s+(?=[A-Z])', text)\n",
        "\n",
        "    # Restore abbreviations and clean up\n",
        "    sentences = [s.replace('U_S_', 'U.S.').replace('e_g_', 'e.g.').replace('i_e_', 'i.e.').strip()\n",
        "                 for s in sentences]\n",
        "\n",
        "    # Filter out very short fragments (likely formatting artifacts)\n",
        "    sentences = [s for s in sentences if len(s) > 20]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 3: Sentence Segmentation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "segmented_data = {}\n",
        "\n",
        "for filename, content in cleaned_statements.items():\n",
        "    sentences = segment_into_sentences(content)\n",
        "    segmented_data[filename] = sentences\n",
        "\n",
        "    # Extract date from filename\n",
        "    date_match = re.search(r'(\\d{4}_\\d{2}_\\d{2})', filename)\n",
        "    date_str = date_match.group(1) if date_match else filename\n",
        "\n",
        "    print(f\"{date_str}: {len(sentences)} sentences\")\n",
        "\n",
        "    # Show first sentence as preview\n",
        "    if sentences:\n",
        "        preview = sentences[0][:100] + \"...\" if len(sentences[0]) > 100 else sentences[0]\n",
        "        print(f\"  Preview: {preview}\")\n",
        "\n",
        "print(f\"\\n‚úì Segmented {len(segmented_data)} statements into sentences\")\n",
        "print()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 4: Create Model-Ready DataFrame\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 4: Creating Model-Ready Dataset\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Build dataset with metadata\n",
        "dataset_rows = []\n",
        "\n",
        "for filename, sentences in segmented_data.items():\n",
        "    # Extract date from filename\n",
        "    date_match = re.search(r'(\\d{4})_(\\d{2})_(\\d{2})', filename)\n",
        "    if date_match:\n",
        "        year, month, day = date_match.groups()\n",
        "        date_str = f\"{year}-{month}-{day}\"\n",
        "    else:\n",
        "        date_str = \"unknown\"\n",
        "\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        dataset_rows.append({\n",
        "            'statement_date': date_str,\n",
        "            'filename': filename,\n",
        "            'sentence_id': idx,\n",
        "            'text': sentence,\n",
        "            'char_length': len(sentence),\n",
        "            'word_count': len(sentence.split())\n",
        "        })\n",
        "\n",
        "# Create DataFrame\n",
        "df_fomc = pd.DataFrame(dataset_rows)\n",
        "\n",
        "# Display summary statistics\n",
        "print(f\"Total sentences: {len(df_fomc)}\")\n",
        "print(f\"Date range: {df_fomc['statement_date'].min()} to {df_fomc['statement_date'].max()}\")\n",
        "print(f\"\\nDataset shape: {df_fomc.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_fomc.head())\n",
        "\n",
        "print(\"\\nüìä Text Length Statistics:\")\n",
        "print(df_fomc[['char_length', 'word_count']].describe())\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 5: Save Processed Data\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 5: Saving Processed Data\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save as CSV\n",
        "csv_path = PROCESSED_DATA_DIR / \"fomc_2024_2025_sentences.csv\"\n",
        "df_fomc.to_csv(csv_path, index=False)\n",
        "print(f\"‚úì Saved CSV: {csv_path}\")\n",
        "\n",
        "# Save as JSON (useful for some NLP workflows)\n",
        "json_path = PROCESSED_DATA_DIR / \"fomc_2024_2025_sentences.json\"\n",
        "df_fomc.to_json(json_path, orient='records', indent=2)\n",
        "print(f\"‚úì Saved JSON: {json_path}\")\n",
        "\n",
        "# Save summary metadata\n",
        "metadata = {\n",
        "    'processing_date': datetime.now().isoformat(),\n",
        "    'num_statements': len(segmented_data),\n",
        "    'num_sentences': len(df_fomc),\n",
        "    'date_range': {\n",
        "        'start': df_fomc['statement_date'].min(),\n",
        "        'end': df_fomc['statement_date'].max()\n",
        "    },\n",
        "    'files_processed': list(raw_statements.keys())\n",
        "}\n",
        "\n",
        "metadata_path = PROCESSED_DATA_DIR / \"processing_metadata.json\"\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"‚úì Saved metadata: {metadata_path}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 6: Data Quality Checks\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 6: Data Quality Checks\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check 1: No empty sentences\n",
        "empty_count = (df_fomc['text'].str.len() == 0).sum()\n",
        "print(f\"Empty sentences: {empty_count}\")\n",
        "\n",
        "# Check 2: Date parsing success\n",
        "unknown_dates = (df_fomc['statement_date'] == 'unknown').sum()\n",
        "print(f\"Unknown dates: {unknown_dates}\")\n",
        "\n",
        "# Check 3: Sentence length distribution\n",
        "print(\"\\nSentence length (words) distribution:\")\n",
        "print(df_fomc['word_count'].describe())\n",
        "\n",
        "# Check 4: Look for potential issues\n",
        "print(\"\\nSample of shortest sentences (potential artifacts):\")\n",
        "print(df_fomc.nsmallest(3, 'word_count')[['statement_date', 'text']])\n",
        "\n",
        "print(\"\\nSample of longest sentences (check for segmentation errors):\")\n",
        "print(df_fomc.nlargest(3, 'word_count')[['statement_date', 'text', 'word_count']])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY & NEXT STEPS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úì DATA COLLECTION & CLEANING COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\"\"\n",
        "üìã Summary:\n",
        "  ‚Ä¢ Processed {len(raw_statements)} FOMC statements from 2024-2025\n",
        "  ‚Ä¢ Extracted {len(df_fomc)} sentences total\n",
        "  ‚Ä¢ Output saved to: {PROCESSED_DATA_DIR}\n",
        "\n",
        "üìÅ Output Files:\n",
        "  1. fomc_2024_2025_sentences.csv - Main dataset\n",
        "  2. fomc_2024_2025_sentences.json - JSON format\n",
        "  3. processing_metadata.json - Processing details\n",
        "\n",
        "üîç Data Quality:\n",
        "  ‚Ä¢ Average sentence length: {df_fomc['word_count'].mean():.1f} words\n",
        "  ‚Ä¢ Min: {df_fomc['word_count'].min()} words\n",
        "  ‚Ä¢ Max: {df_fomc['word_count'].max()} words\n",
        "\n",
        "üìù Next Steps:\n",
        "  1. ‚úì Data collected and cleaned\n",
        "  2. ‚Üí Proceed to notebooks/2_ood_generalization_cases.ipynb\n",
        "     Load FOMC-RoBERTa model and run inference on these sentences\n",
        "  3. ‚Üí Identify failure cases where model misinterprets 2024-2025 narratives\n",
        "  4. ‚Üí Connect sentiment scores to market data in notebook 3\n",
        "\n",
        "üí° Notes for FSIL Review:\n",
        "  ‚Ä¢ This pipeline handles raw Fed website text ‚Üí model-ready sentences\n",
        "  ‚Ä¢ Segmentation is intentionally simple (rule-based) to avoid introducing bias\n",
        "  ‚Ä¢ More sophisticated NLP parsing (spaCy, etc.) can be added if needed\n",
        "  ‚Ä¢ Current approach prioritizes transparency and reproducibility\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Ready for OOD Analysis!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjs5Td5mIOEz",
        "outputId": "467bf2c2-3805-4ea5-fcf1-bc312d195feb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Notebook 1: Data Collection & Cleaning\n",
            "Target: 2024-2025 FOMC Statements (OOD Data)\n",
            "============================================================\n",
            "\n",
            "üìÅ Raw data directory: /content/fomc-ood-stress-test/data/raw\n",
            "üìÅ Processed data directory: /content/fomc-ood-stress-test/data/processed\n",
            "\n",
            "Found 3 FOMC statement files:\n",
            "  ‚úì 2024-01-31: 1883 characters\n",
            "  ‚úì 2024-03-20: 1849 characters\n",
            "  ‚úì 2024-05-01: 2049 characters\n",
            "\n",
            "‚úì Loaded 3 statements successfully\n",
            "\n",
            "============================================================\n",
            "STEP 2: Cleaning Text\n",
            "============================================================\n",
            "fomc_2024_01_31_statement.txt: 5 ‚Üí 4 lines\n",
            "fomc_2024_03_20_statement.txt: 5 ‚Üí 4 lines\n",
            "fomc_2024_05_01_statement.txt: 5 ‚Üí 4 lines\n",
            "\n",
            "‚úì Cleaned 3 statements\n",
            "\n",
            "============================================================\n",
            "STEP 3: Sentence Segmentation\n",
            "============================================================\n",
            "2024_01_31: 14 sentences\n",
            "  Preview: Recent indicators suggest that economic activity has been expanding at a solid pace\n",
            "2024_03_20: 14 sentences\n",
            "  Preview: Recent indicators suggest that economic activity has been expanding at a solid pace\n",
            "2024_05_01: 16 sentences\n",
            "  Preview: Recent indicators suggest that economic activity has continued to expand at a solid pace\n",
            "\n",
            "‚úì Segmented 3 statements into sentences\n",
            "\n",
            "============================================================\n",
            "STEP 4: Creating Model-Ready Dataset\n",
            "============================================================\n",
            "Total sentences: 44\n",
            "Date range: 2024-01-31 to 2024-05-01\n",
            "\n",
            "Dataset shape: (44, 6)\n",
            "\n",
            "First few rows:\n",
            "  statement_date                       filename  sentence_id  \\\n",
            "0     2024-01-31  fomc_2024_01_31_statement.txt            0   \n",
            "1     2024-01-31  fomc_2024_01_31_statement.txt            1   \n",
            "2     2024-01-31  fomc_2024_01_31_statement.txt            2   \n",
            "3     2024-01-31  fomc_2024_01_31_statement.txt            3   \n",
            "4     2024-01-31  fomc_2024_01_31_statement.txt            4   \n",
            "\n",
            "                                                text  char_length  word_count  \n",
            "0  Recent indicators suggest that economic activi...           83          13  \n",
            "1  Job gains have moderated since early last year...          108          18  \n",
            "2  Inflation has eased over the past year but rem...           59          10  \n",
            "3  The Committee seeks to achieve maximum employm...          108          19  \n",
            "4  The Committee judges that the risks to achievi...          114          18  \n",
            "\n",
            "üìä Text Length Statistics:\n",
            "       char_length  word_count\n",
            "count    44.000000   44.000000\n",
            "mean    129.454545   20.204545\n",
            "std      45.600109    6.486406\n",
            "min      59.000000   10.000000\n",
            "25%      91.750000   14.000000\n",
            "50%     121.000000   19.000000\n",
            "75%     172.000000   26.000000\n",
            "max     220.000000   32.000000\n",
            "\n",
            "============================================================\n",
            "STEP 5: Saving Processed Data\n",
            "============================================================\n",
            "‚úì Saved CSV: /content/fomc-ood-stress-test/data/processed/fomc_2024_2025_sentences.csv\n",
            "‚úì Saved JSON: /content/fomc-ood-stress-test/data/processed/fomc_2024_2025_sentences.json\n",
            "‚úì Saved metadata: /content/fomc-ood-stress-test/data/processed/processing_metadata.json\n",
            "\n",
            "============================================================\n",
            "STEP 6: Data Quality Checks\n",
            "============================================================\n",
            "Empty sentences: 0\n",
            "Unknown dates: 0\n",
            "\n",
            "Sentence length (words) distribution:\n",
            "count    44.000000\n",
            "mean     20.204545\n",
            "std       6.486406\n",
            "min      10.000000\n",
            "25%      14.000000\n",
            "50%      19.000000\n",
            "75%      26.000000\n",
            "max      32.000000\n",
            "Name: word_count, dtype: float64\n",
            "\n",
            "Sample of shortest sentences (potential artifacts):\n",
            "   statement_date                                               text\n",
            "2      2024-01-31  Inflation has eased over the past year but rem...\n",
            "16     2024-03-20  Inflation has eased over the past year but rem...\n",
            "30     2024-05-01  Inflation has eased over the past year but rem...\n",
            "\n",
            "Sample of longest sentences (check for segmentation errors):\n",
            "   statement_date                                               text  \\\n",
            "40     2024-05-01  The Committee will maintain the monthly redemp...   \n",
            "39     2024-05-01  Beginning in June, the Committee will slow the...   \n",
            "7      2024-01-31  In considering any adjustments to the target r...   \n",
            "\n",
            "    word_count  \n",
            "40          32  \n",
            "39          30  \n",
            "7           28  \n",
            "\n",
            "============================================================\n",
            "‚úì DATA COLLECTION & CLEANING COMPLETE\n",
            "============================================================\n",
            "\n",
            "üìã Summary:\n",
            "  ‚Ä¢ Processed 3 FOMC statements from 2024-2025\n",
            "  ‚Ä¢ Extracted 44 sentences total\n",
            "  ‚Ä¢ Output saved to: /content/fomc-ood-stress-test/data/processed\n",
            "\n",
            "üìÅ Output Files:\n",
            "  1. fomc_2024_2025_sentences.csv - Main dataset\n",
            "  2. fomc_2024_2025_sentences.json - JSON format\n",
            "  3. processing_metadata.json - Processing details\n",
            "\n",
            "üîç Data Quality:\n",
            "  ‚Ä¢ Average sentence length: 20.2 words\n",
            "  ‚Ä¢ Min: 10 words\n",
            "  ‚Ä¢ Max: 32 words\n",
            "\n",
            "üìù Next Steps:\n",
            "  1. ‚úì Data collected and cleaned\n",
            "  2. ‚Üí Proceed to notebooks/2_ood_generalization_cases.ipynb\n",
            "     Load FOMC-RoBERTa model and run inference on these sentences\n",
            "  3. ‚Üí Identify failure cases where model misinterprets 2024-2025 narratives\n",
            "  4. ‚Üí Connect sentiment scores to market data in notebook 3\n",
            "\n",
            "üí° Notes for FSIL Review:\n",
            "  ‚Ä¢ This pipeline handles raw Fed website text ‚Üí model-ready sentences\n",
            "  ‚Ä¢ Segmentation is intentionally simple (rule-based) to avoid introducing bias\n",
            "  ‚Ä¢ More sophisticated NLP parsing (spaCy, etc.) can be added if needed\n",
            "  ‚Ä¢ Current approach prioritizes transparency and reproducibility\n",
            "\n",
            "\n",
            "============================================================\n",
            "Ready for OOD Analysis!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8P6Qnr5_RXp9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}